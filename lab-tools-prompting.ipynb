{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77f66dbe-192b-471c-9cb8-e9b365e61bbb",
   "metadata": {},
   "source": [
    "# Lab | Tools prompting\n",
    "\n",
    "**Replace the existing two tools decorators, by creating 3 new ones and adjust the prompts accordingly**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b94240",
   "metadata": {},
   "source": [
    "### How to add ad-hoc tool calling capability to LLMs and Chat Models\n",
    "\n",
    ":::{.callout-caution}\n",
    "\n",
    "Some models have been fine-tuned for tool calling and provide a dedicated API for tool calling. Generally, such models are better at tool calling than non-fine-tuned models, and are recommended for use cases that require tool calling. Please see the [how to use a chat model to call tools](https://python.langchain.com/docs/how_to/tool_calling/) guide for more information.\n",
    "\n",
    "In this guide, we'll see how to add **ad-hoc** tool calling support to a chat model. This is an alternative method to invoke tools if you're using a model that does not natively support tool calling.\n",
    "\n",
    "We'll do this by simply writing a prompt that will get the model to invoke the appropriate tools. Here's a diagram of the logic:\n",
    "\n",
    "<br>\n",
    "\n",
    "![chain](https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/tool_chain.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a22cb8-19e7-450a-9d1b-6848d2c81cd1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll need to install the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c556c5e-b785-428b-8e7d-efd34a2a1adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade --quiet langchain langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897bc01e-cc2b-4400-8a64-db4aa56085d3",
   "metadata": {},
   "source": [
    "If you'd like to use LangSmith, uncomment the below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416da17c",
   "metadata": {},
   "source": [
    "smith.langchain.com    ===    api key: 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5efb4170-b95b-4d29-8f57-09509f3ba6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec6409b-21e5-4d0a-8a46-c4ef0b055dd3",
   "metadata": {},
   "source": [
    "You can select any of the given models for this how-to guide. Keep in mind that most of these models already [support native tool calling](https://python.langchain.com/docs/integrations/chat), so using the prompting strategy shown here doesn't make sense for these models, and instead you should follow the [how to use a chat model to call tools](https://python.langchain.com/docs/how_to/tool_calling/) guide.\n",
    "\n",
    "```{=mdx}\n",
    "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
    "\n",
    "<ChatModelTabs openaiParams={`model=\"gpt-4\"`} />\n",
    "```\n",
    "\n",
    "To illustrate the idea, we'll use `phi3` via Ollama, which does **NOT** have native support for tool calling. If you'd like to use `Ollama` as well follow [these instructions](https://python.langchain.com/docs/integrations/chat/ollama)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "424be968-2806-4d1a-a6aa-5499ae20fac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Latif-Calder贸n\\AppData\\Local\\Temp\\ipykernel_30832\\1427064109.py:3: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  model = Ollama(model=\"phi3\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "model = Ollama(model=\"phi3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68946881",
   "metadata": {},
   "source": [
    "## Create a tool\n",
    "\n",
    "First, let's create an `add` and `multiply` tools. For more information on creating custom tools, please see [this guide](https://python.langchain.com/docs/how_to/custom_tools/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4548e6fa-0f9b-4d7a-8fa5-66cec0350e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n",
      "multiply\n",
      "Multiply two numbers together.\n",
      "{'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}\n",
      "--\n",
      "add\n",
      "Add two numbers.\n",
      "{'x': {'title': 'X', 'type': 'integer'}, 'y': {'title': 'Y', 'type': 'integer'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(x: float, y: float) -> float:\n",
    "    \"\"\"Multiply two numbers together.\"\"\"\n",
    "    return x * y\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(x: int, y: int) -> int:\n",
    "    \"Add two numbers.\"\n",
    "    return x + y\n",
    "\n",
    "\n",
    "tools = [multiply, add]\n",
    "\n",
    "# Let's inspect the tools\n",
    "for t in tools:\n",
    "    print(\"--\")\n",
    "    print(t.name)\n",
    "    print(t.description)\n",
    "    print(t.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be77e780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiply.invoke({\"x\": 4, \"y\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dd690e-e54d-4209-91a4-181f69a452ac",
   "metadata": {},
   "source": [
    "## Creating our prompt\n",
    "\n",
    "We'll want to write a prompt that specifies the tools the model has access to, the arguments to those tools, and the desired output format of the model. In this case we'll instruct it to output a JSON blob of the form `{\"name\": \"...\", \"arguments\": {...}}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2063b564-25ca-4729-a45f-ba4633175b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiply(x: float, y: float) -> float - Multiply two numbers together.\n",
      "add(x: int, y: int) -> int - Add two numbers.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import render_text_description\n",
    "\n",
    "rendered_tools = render_text_description(tools)\n",
    "print(rendered_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f02f1dce-76e7-4ca9-9bac-5af496131fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"\\\n",
    "You are an assistant that has access to the following set of tools. \n",
    "Here are the names and descriptions for each tool:\n",
    "\n",
    "{rendered_tools}\n",
    "\n",
    "Given the user input, return the name and input of the tool to use. \n",
    "Return your response as a JSON blob with 'name' and 'arguments' keys.\n",
    "\n",
    "The `arguments` should be a dictionary, with keys corresponding \n",
    "to the argument names and the values corresponding to the requested values.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt), (\"user\", \"{input}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8623e03-60eb-4439-b57b-ecbcebc61b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"add\",\n",
      "  \"arguments\": {\n",
      "    \"x\": 3,\n",
      "    \"y\": 1132\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | model\n",
    "message = chain.invoke({\"input\": \"what's 3 plus 1132\"})\n",
    "\n",
    "# Let's take a look at the output from the model\n",
    "# if the model is an LLM (not a chat model), the output will be a string.\n",
    "if isinstance(message, str):\n",
    "    print(message)\n",
    "else:  # Otherwise it's a chat model\n",
    "    print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14df2cd5-b6fa-4b10-892d-e8692c7931e5",
   "metadata": {},
   "source": [
    "## Adding an output parser\n",
    "\n",
    "We'll use the `JsonOutputParser` for parsing our models output to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f129f5bd-127c-4c95-8f34-8f437da7ca8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'multiply', 'arguments': {'x': 13, 'y': 4}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "chain = prompt | model | JsonOutputParser()\n",
    "chain.invoke({\"input\": \"what's thirteen times 4\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f08255-f146-4f4a-be43-5c21c1d3ae83",
   "metadata": {},
   "source": [
    ":::{.callout-important}\n",
    "\n",
    " Amazing!  We now instructed our model on how to **request** that a tool be invoked.\n",
    "\n",
    "Now, let's create some logic to actually run the tool!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e29dd4c-8eb5-457f-92d1-8add076404dc",
   "metadata": {},
   "source": [
    "## Invoking the tool \n",
    "\n",
    "Now that the model can request that a tool be invoked, we need to write a function that can actually invoke \n",
    "the tool.\n",
    "\n",
    "The function will select the appropriate tool by name, and pass to it the arguments chosen by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faee95e0-4095-4310-991f-9e9465c6738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Optional, TypedDict\n",
    "\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "class ToolCallRequest(TypedDict):\n",
    "    \"\"\"A typed dict that shows the inputs into the invoke_tool function.\"\"\"\n",
    "\n",
    "    name: str\n",
    "    arguments: Dict[str, Any]\n",
    "\n",
    "\n",
    "def invoke_tool(\n",
    "    tool_call_request: ToolCallRequest, config: Optional[RunnableConfig] = None\n",
    "):\n",
    "    \"\"\"A function that we can use the perform a tool invocation.\n",
    "\n",
    "    Args:\n",
    "        tool_call_request: a dict that contains the keys name and arguments.\n",
    "            The name must match the name of a tool that exists.\n",
    "            The arguments are the arguments to that tool.\n",
    "        config: This is configuration information that LangChain uses that contains\n",
    "            things like callbacks, metadata, etc.See LCEL documentation about RunnableConfig.\n",
    "\n",
    "    Returns:\n",
    "        output from the requested tool\n",
    "    \"\"\"\n",
    "    tool_name_to_tool = {tool.name: tool for tool in tools}\n",
    "    name = tool_call_request[\"name\"]\n",
    "    requested_tool = tool_name_to_tool[name]\n",
    "    return requested_tool.invoke(tool_call_request[\"arguments\"], config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4957532-9e0c-47f6-bb62-0fd789ac1d3e",
   "metadata": {},
   "source": [
    "Let's test this out И!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0ea3b2a-8fb2-4016-83c8-a5d3e78fedbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoke_tool({\"name\": \"multiply\", \"arguments\": {\"x\": 3, \"y\": 5}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715af6e1-935d-4bc0-a3d2-646ecf8a329b",
   "metadata": {},
   "source": [
    "## Let's put it together\n",
    "\n",
    "Let's put it together into a chain that creates a calculator with add and multiplication capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0555b384-fde6-4404-86e0-7ea199003d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53.83784653"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | JsonOutputParser() | invoke_tool\n",
    "chain.invoke({\"input\": \"what's thirteen times 4.14137281\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a9c5aa-f60a-4017-af6f-1ff6e04bfb61",
   "metadata": {},
   "source": [
    "## Returning tool inputs\n",
    "\n",
    "It can be helpful to return not only tool outputs but also tool inputs. We can easily do this with LCEL by `RunnablePassthrough.assign`-ing the tool output. This will take whatever the input is to the RunnablePassrthrough components (assumed to be a dictionary) and add a key to it while still passing through everything that's currently in the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45404406-859d-4caa-8b9d-5838162c80a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunnablePassthrough\n\u001b[0;32m      3\u001b[0m chain \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m----> 4\u001b[0m     prompt \u001b[38;5;241m|\u001b[39m model \u001b[38;5;241m|\u001b[39m JsonOutputParser() \u001b[38;5;241m|\u001b[39m RunnablePassthrough\u001b[38;5;241m.\u001b[39massign(output\u001b[38;5;241m=\u001b[39minvoke_tool)\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      6\u001b[0m chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms thirteen times 4.14137281\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prompt' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    prompt | model | JsonOutputParser() | RunnablePassthrough.assign(output=invoke_tool)\n",
    ")\n",
    "chain.invoke({\"input\": \"what's thirteen times 4.14137281\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1797fe82-ea35-4cba-834a-1caf9740d184",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "This how-to guide shows the \"happy path\" when the model correctly outputs all the required tool information.\n",
    "\n",
    "In reality, if you're using more complex tools, you will start encountering errors from the model, especially for models that have not been fine tuned for tool calling and for less capable models.\n",
    "\n",
    "You will need to be prepared to add strategies to improve the output from the model; e.g.,\n",
    "\n",
    "1. Provide few shot examples.\n",
    "2. Add error handling (e.g., catch the exception and feed it back to the LLM to ask it to correct its previous output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cabf40b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multiply' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubtract two numbers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m-\u001b[39m y\n\u001b[1;32m----> 8\u001b[0m tools \u001b[38;5;241m=\u001b[39m [multiply, add, subtract, divide]\n\u001b[0;32m     10\u001b[0m rendered_tools \u001b[38;5;241m=\u001b[39m render_text_description(tools)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(rendered_tools)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'multiply' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def subtract(x: int, y: int) -> int:\n",
    "    \"Subtract two numbers.\"\n",
    "    return x - y\n",
    "\n",
    "tools = [multiply, add, subtract, divide]\n",
    "\n",
    "rendered_tools = render_text_description(tools)\n",
    "print(rendered_tools)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "631726ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rendered_tools' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m system_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124mYou are an assistant that has access to the following set of tools.\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124mHere are the names and descriptions for each tool:\u001b[39m\n\u001b[0;32m      4\u001b[0m \n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mrendered_tools\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;124mGiven the user input, return the name and input of the tool to use.\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124mReturn your response as a JSON blob with \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marguments\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keys.\u001b[39m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m \u001b[38;5;124mThe `arguments` should be a dictionary, with keys corresponding\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124mto the argument names and the values corresponding to the requested values.\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rendered_tools' is not defined"
     ]
    }
   ],
   "source": [
    "system_prompt = f\"\"\"\\\n",
    "You are an assistant that has access to the following set of tools.\n",
    "Here are the names and descriptions for each tool:\n",
    "\n",
    "{rendered_tools}\n",
    "\n",
    "Given the user input, return the name and input of the tool to use.\n",
    "Return your response as a JSON blob with 'name' and 'arguments' keys.\n",
    "\n",
    "The `arguments` should be a dictionary, with keys corresponding\n",
    "to the argument names and the values corresponding to the requested values.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0faa985",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatPromptTemplate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages(\n\u001b[0;32m      2\u001b[0m     [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, system_prompt), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{input}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      5\u001b[0m chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m model \u001b[38;5;241m|\u001b[39m JsonOutputParser() \u001b[38;5;241m|\u001b[39m RunnablePassthrough\u001b[38;5;241m.\u001b[39massign(\n\u001b[0;32m      6\u001b[0m     output\u001b[38;5;241m=\u001b[39minvoke_tool\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms thirteen times 4.14137281\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ChatPromptTemplate' is not defined"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt), (\"user\", \"{input}\")]\n",
    ")\n",
    "\n",
    "chain = prompt | model | JsonOutputParser() | RunnablePassthrough.assign(\n",
    "    output=invoke_tool\n",
    ")\n",
    "chain.invoke({\"input\": \"what's thirteen times 4.14137281\"})\n",
    "\n",
    "message = chain.invoke({\"input\": \"what's thirteen times 4.14137281\"})\n",
    "if isinstance(message, str):\n",
    "    print(message)\n",
    "else:  # Otherwise it's a chat model\n",
    "    print(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd7039a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Input to ChatPromptTemplate is missing variables {\"\\'name\\'\"}.  Expected: [\"\\'name\\'\", \\'input\\'] Received: [\\'input\\', \\'name\\']\\nNote: if you intended {\\'name\\'} to be part of the string and not a variable, please escape it with double curly braces like: \\'{{\\'name\\'}}\\'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT '",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 36\u001b[0m\n\u001b[0;32m     31\u001b[0m chain \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     32\u001b[0m     prompt \u001b[38;5;241m|\u001b[39m model \u001b[38;5;241m|\u001b[39m JsonOutputParser() \u001b[38;5;241m|\u001b[39m RunnablePassthrough\u001b[38;5;241m.\u001b[39massign(output\u001b[38;5;241m=\u001b[39minvoke_tool_with_error_handling)\n\u001b[0;32m     33\u001b[0m )\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Test the updated chain with the required 'name' variable\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms thirteen times 4.14137281\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour_name\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[1;32mc:\\Users\\Latif-Calder贸n\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3014\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3012\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m   3013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 3014\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3015\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3016\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\Latif-Calder贸n\\anaconda3\\Lib\\site-packages\\langchain_core\\prompts\\base.py:210\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags:\n\u001b[0;32m    209\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags\n\u001b[1;32m--> 210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_prompt_with_error_handling,\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    213\u001b[0m     config,\n\u001b[0;32m    214\u001b[0m     run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    215\u001b[0m     serialized\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[0;32m    216\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Latif-Calder贸n\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1914\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[0;32m   1910\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m   1911\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m   1912\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1913\u001b[0m         Output,\n\u001b[1;32m-> 1914\u001b[0m         context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m   1915\u001b[0m             call_func_with_variable_args,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1916\u001b[0m             func,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1917\u001b[0m             \u001b[38;5;28minput\u001b[39m,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1918\u001b[0m             config,\n\u001b[0;32m   1919\u001b[0m             run_manager,\n\u001b[0;32m   1920\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1921\u001b[0m         ),\n\u001b[0;32m   1922\u001b[0m     )\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1924\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\Latif-Calder贸n\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\config.py:396\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    395\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Latif-Calder贸n\\anaconda3\\Lib\\site-packages\\langchain_core\\prompts\\base.py:184\u001b[0m, in \u001b[0;36mBasePromptTemplate._format_prompt_with_error_handling\u001b[1;34m(self, inner_input)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_format_prompt_with_error_handling\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner_input: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[1;32m--> 184\u001b[0m     _inner_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_input(inner_input)\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_prompt(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_inner_input)\n",
      "File \u001b[1;32mc:\\Users\\Latif-Calder贸n\\anaconda3\\Lib\\site-packages\\langchain_core\\prompts\\base.py:178\u001b[0m, in \u001b[0;36mBasePromptTemplate._validate_input\u001b[1;34m(self, inner_input)\u001b[0m\n\u001b[0;32m    172\u001b[0m     example_key \u001b[38;5;241m=\u001b[39m missing\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m    173\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    174\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNote: if you intended \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m to be part of the string\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and not a variable, please escape it with double curly braces like: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    176\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    177\u001b[0m     )\n\u001b[1;32m--> 178\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m    179\u001b[0m         create_message(message\u001b[38;5;241m=\u001b[39mmsg, error_code\u001b[38;5;241m=\u001b[39mErrorCode\u001b[38;5;241m.\u001b[39mINVALID_PROMPT_INPUT)\n\u001b[0;32m    180\u001b[0m     )\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inner_input\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Input to ChatPromptTemplate is missing variables {\"\\'name\\'\"}.  Expected: [\"\\'name\\'\", \\'input\\'] Received: [\\'input\\', \\'name\\']\\nNote: if you intended {\\'name\\'} to be part of the string and not a variable, please escape it with double curly braces like: \\'{{\\'name\\'}}\\'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT '"
     ]
    }
   ],
   "source": [
    "# ...existing code...\n",
    "\n",
    "# Few-shot examples\n",
    "few_shot_examples = [\n",
    "    {\"input\": \"what's 2 plus 2\", \"output\": {\"name\": \"add\", \"arguments\": {\"x\": 2, \"y\": 2}}},\n",
    "    {\"input\": \"multiply 3 and 4\", \"output\": {\"name\": \"multiply\", \"arguments\": {\"x\": 3, \"y\": 4}}},\n",
    "]\n",
    "\n",
    "# Update the prompt to include few-shot examples\n",
    "few_shot_prompt = system_prompt + \"\\n\\nHere are some examples:\\n\"\n",
    "for example in few_shot_examples:\n",
    "    few_shot_prompt += f\"User: {example['input']}\\nAssistant: {example['output']}\\n\"\n",
    "\n",
    "# Include the 'name' variable in the user message\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", few_shot_prompt), (\"user\", \"{input}\")]\n",
    ")\n",
    "\n",
    "# Error handling\n",
    "def invoke_tool_with_error_handling(tool_call_request: ToolCallRequest, config: Optional[RunnableConfig] = None):\n",
    "    try:\n",
    "        return invoke_tool(tool_call_request, config)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        correction_prompt = f\"An error occurred: {e}. Please correct your previous output.\"\n",
    "        correction_chain = prompt | model | JsonOutputParser()\n",
    "        correction_message = correction_chain.invoke({\"input\": correction_prompt, \"name\": \"your_name\"})\n",
    "        return correction_message\n",
    "\n",
    "# Update the chain to use the new invoke_tool_with_error_handling function\n",
    "chain = (\n",
    "    prompt | model | JsonOutputParser() | RunnablePassthrough.assign(output=invoke_tool_with_error_handling)\n",
    ")\n",
    "\n",
    "# Test the updated chain with the required 'name' variable\n",
    "chain.invoke({\"input\": \"what's thirteen times 4.14137281\", \"name\": \"your_name\"})\n",
    "# ...existing code...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0b9f663",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unexpected message type: name. Use one of 'human', 'user', 'ai', 'assistant', or 'system'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m     few_shot_prompt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAssistant: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Include the 'name' variable in the prompt template\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages(\n\u001b[0;32m     14\u001b[0m     [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, few_shot_prompt), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{input}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{name}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Error handling\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke_tool_with_error_handling\u001b[39m(tool_call_request: ToolCallRequest, config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\Latif-Calder贸n\\anaconda3\\Lib\\site-packages\\langchain_core\\prompts\\chat.py:1204\u001b[0m, in \u001b[0;36mChatPromptTemplate.from_messages\u001b[1;34m(cls, messages, template_format)\u001b[0m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_messages\u001b[39m(\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   1167\u001b[0m     messages: Sequence[MessageLikeRepresentation],\n\u001b[0;32m   1168\u001b[0m     template_format: PromptTemplateFormat \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf-string\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1169\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatPromptTemplate:\n\u001b[0;32m   1170\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a chat prompt template from a variety of message formats.\u001b[39;00m\n\u001b[0;32m   1171\u001b[0m \n\u001b[0;32m   1172\u001b[0m \u001b[38;5;124;03m    Examples:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1202\u001b[0m \u001b[38;5;124;03m        a chat prompt template.\u001b[39;00m\n\u001b[0;32m   1203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(messages, template_format\u001b[38;5;241m=\u001b[39mtemplate_format)\n",
      "File \u001b[1;32mc:\\Users\\Latif-Calder贸n\\anaconda3\\Lib\\site-packages\\langchain_core\\prompts\\chat.py:1002\u001b[0m, in \u001b[0;36mChatPromptTemplate.__init__\u001b[1;34m(self, messages, template_format, **kwargs)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    949\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    950\u001b[0m     messages: Sequence[MessageLikeRepresentation],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    953\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    954\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    955\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a chat prompt template from a variety of message formats.\u001b[39;00m\n\u001b[0;32m    956\u001b[0m \n\u001b[0;32m    957\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    999\u001b[0m \n\u001b[0;32m   1000\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m     _messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m-> 1002\u001b[0m         _convert_to_message(message, template_format) \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[0;32m   1003\u001b[0m     ]\n\u001b[0;32m   1005\u001b[0m     \u001b[38;5;66;03m# Automatically infer input variables from messages\u001b[39;00m\n\u001b[0;32m   1006\u001b[0m     input_vars: \u001b[38;5;28mset\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Latif-Calder贸n\\anaconda3\\Lib\\site-packages\\langchain_core\\prompts\\chat.py:1470\u001b[0m, in \u001b[0;36m_convert_to_message\u001b[1;34m(message, template_format)\u001b[0m\n\u001b[0;32m   1468\u001b[0m message_type_str, template \u001b[38;5;241m=\u001b[39m message\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message_type_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 1470\u001b[0m     _message \u001b[38;5;241m=\u001b[39m _create_template_from_message_type(\n\u001b[0;32m   1471\u001b[0m         message_type_str, template, template_format\u001b[38;5;241m=\u001b[39mtemplate_format\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1474\u001b[0m     _message \u001b[38;5;241m=\u001b[39m message_type_str(\n\u001b[0;32m   1475\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\n\u001b[0;32m   1476\u001b[0m             cast(\u001b[38;5;28mstr\u001b[39m, template), template_format\u001b[38;5;241m=\u001b[39mtemplate_format\n\u001b[0;32m   1477\u001b[0m         )\n\u001b[0;32m   1478\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Latif-Calder贸n\\anaconda3\\Lib\\site-packages\\langchain_core\\prompts\\chat.py:1425\u001b[0m, in \u001b[0;36m_create_template_from_message_type\u001b[1;34m(message_type, template, template_format)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1421\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1422\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected message type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Use one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mai\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1424\u001b[0m     )\n\u001b[1;32m-> 1425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m message\n",
      "\u001b[1;31mValueError\u001b[0m: Unexpected message type: name. Use one of 'human', 'user', 'ai', 'assistant', or 'system'."
     ]
    }
   ],
   "source": [
    "# Few-shot examples\n",
    "few_shot_examples = [\n",
    "    {\"input\": \"what's 2 plus 2\", \"output\": {\"name\": \"add\", \"arguments\": {\"x\": 2, \"y\": 2}}},\n",
    "    {\"input\": \"multiply 3 and 4\", \"output\": {\"name\": \"multiply\", \"arguments\": {\"x\": 3, \"y\": 4}}},\n",
    "]\n",
    "\n",
    "## Update the prompt to include few-shot examples and the 'name' variable\n",
    "few_shot_prompt = system_prompt + \"\\n\\nHere are some examples:\\n\"\n",
    "for example in few_shot_examples:\n",
    "    few_shot_prompt += f\"User: {example['input']}\\nAssistant: {example['output']}\\n\"\n",
    "\n",
    "# Include the 'name' variable in the prompt template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", few_shot_prompt), (\"user\", \"{input}\"), (\"name\", \"{name}\")]\n",
    ")\n",
    "\n",
    "# Error handling\n",
    "def invoke_tool_with_error_handling(tool_call_request: ToolCallRequest, config: Optional[RunnableConfig] = None):\n",
    "    try:\n",
    "        return invoke_tool(tool_call_request, config)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        correction_prompt = f\"An error occurred: {e}. Please correct your previous output.\"\n",
    "        correction_chain = prompt | model | JsonOutputParser()\n",
    "        correction_message = correction_chain.invoke({\"input\": correction_prompt, \"name\": \"your_name\"})\n",
    "        return correction_message\n",
    "\n",
    "# Update the chain to use the new invoke_tool_with_error_handling function\n",
    "chain = (\n",
    "    prompt | model | JsonOutputParser() | RunnablePassthrough.assign(output=invoke_tool_with_error_handling)\n",
    ")\n",
    "\n",
    "# Test the updated chain with the required 'name' variable\n",
    "chain.invoke({\"input\": \"what's thirteen times 4.14137281\", \"name\": \"Fido\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe19c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
